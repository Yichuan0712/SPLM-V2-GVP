{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "293abfc9",
      "metadata": {
        "id": "293abfc9"
      },
      "source": [
        "# **S-PLM v2: Quickstart**\n",
        "\n",
        "This notebook is a **usage example** of **S-PLM v2**.\n",
        "\n",
        "* **Purpose:**\n",
        "\n",
        "    1. Process PDB structures into the standardized inputs expected by our model.\n",
        "    \n",
        "    2. Generate **protein-level** and **residue-level** embeddings.\n",
        "    \n",
        "    3. Run sample evaluations and export metrics/logs.\n",
        "* **Checkpoint:** An S-PLM v2 `.pth` checkpoint. Download from the provided [SharePoint link](https://mailmissouri-my.sharepoint.com/:u:/g/personal/wangdu_umsystem_edu/EUZ74fO3NOxHjTvc6uvKwDsB5fELaaw-oiPHFU9CJky_hg?e=4phwL0).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment Setup**\n",
        "\n",
        "We **recommend** using an NVIDIA **A100** in Colab; other GPUs/CPU will work but may be slower or run into memory limits.\n"
      ],
      "metadata": {
        "id": "J0Zx6LDn_ZEu"
      },
      "id": "J0Zx6LDn_ZEu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf665391",
      "metadata": {
        "id": "cf665391"
      },
      "outputs": [],
      "source": [
        "# Clone S-PLM\n",
        "!git clone -q https://github.com/Yichuan0712/SPLM-V2-GVP /content/SPLMv2\n",
        "\n",
        "# Install minimal deps\n",
        "!pip install 'git+https://github.com/facebookresearch/esm.git' -q\n",
        "!pip install 'git+https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup' -q\n",
        "!pip install biopython -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"torch==2.5.0\" \"torchvision==0.20.0\" \"torchaudio==2.5.0\" \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "import torch\n",
        "TORCH = \"2.5.0\"\n",
        "CUDA = \"cu\" + torch.version.cuda.replace(\".\", \"\")\n",
        "whl_url = f\"https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\"\n",
        "print(\"Using wheel URL:\", whl_url)\n",
        "!pip install -q pyg_lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "    -f {whl_url}\n",
        "!pip install -q torch-geometric"
      ],
      "metadata": {
        "id": "9LGjoCZrUEKO"
      },
      "id": "9LGjoCZrUEKO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3efac75f",
      "metadata": {
        "id": "3efac75f"
      },
      "source": [
        "### **Prepare Checkpoint**\n",
        "\n",
        "1. **Download the model** from the provided **[SharePoint link](https://mailmissouri-my.sharepoint.com/:u:/g/personal/wangdu_umsystem_edu/EUZ74fO3NOxHjTvc6uvKwDsB5fELaaw-oiPHFU9CJky_hg?e=4phwL0)** to your local machine.\n",
        "2. **Upload to your Colab runtime** (Files pane → Upload to session storage), then set:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_PATH = \"/content/checkpoint_0280000_gvp.pth\""
      ],
      "metadata": {
        "id": "U-SK-aJNDnXy"
      },
      "id": "U-SK-aJNDnXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Faster option (recommended):** Mount Google Drive and copy the checkpoint from Drive into the Colab runtime.\n"
      ],
      "metadata": {
        "id": "YLdBSzFSDqcc"
      },
      "id": "YLdBSzFSDqcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba12f7e",
      "metadata": {
        "id": "1ba12f7e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "import os, shutil\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "shutil.copy(\"/content/drive/MyDrive/checkpoint_0280000_gvp.pth\",\n",
        "            \"/content/checkpoint_0280000_gvp.pth\")\n",
        "CHECKPOINT_PATH = \"/content/checkpoint_0280000_gvp.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate Sequence Embeddings**\n",
        "\n",
        "Use GVP model to generate embeddings from FASTA sequences, with optional truncation and residue-level outputs.\n",
        "\n",
        "* **Standard run:** produces **protein-level** embeddings from `.fasta` to `.pkl`\n",
        "* **Truncated run:** sets `--truncate_inference 1 --max_length_inference 1022` to handle long sequences\n",
        "\n",
        "* **Residue-level run:** adds `--residue_level`\n",
        "\n",
        "**Inputs:** `--input_seq` (FASTA), `--config_path`, `--checkpoint_path`.\n",
        "\n",
        "**Outputs:** pickled embeddings in the working directory (per protein or per residue, depending on flags).\n"
      ],
      "metadata": {
        "id": "q1yTNCuHcBZg"
      },
      "id": "q1yTNCuHcBZg"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/SPLMv2')\n",
        "\n",
        "# standard run\n",
        "!python3 -m utils.generate_seq_embedding --input_seq /content/SPLMv2/dataset/protein.fasta \\\n",
        "  --config_path /content/SPLMv2/configs/config_plddtallweight_noseq_rotary_foldseek.yaml \\\n",
        "  --checkpoint_path /content/checkpoint_0280000_gvp.pth \\\n",
        "  --result_path ./"
      ],
      "metadata": {
        "id": "jrvxaxx2cqYS"
      },
      "id": "jrvxaxx2cqYS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/SPLMv2')\n",
        "\n",
        "# truncate_inference with max_length_inference=1022\n",
        "!python3 -m utils.generate_seq_embedding --input_seq /content/SPLMv2/dataset/protein.fasta \\\n",
        "--config_path /content/SPLMv2/configs/config_plddtallweight_noseq_rotary_foldseek.yaml \\\n",
        "--checkpoint_path /content/checkpoint_0280000_gvp.pth \\\n",
        "--result_path ./ --out_file truncate_protein_embeddings.pkl \\\n",
        "--truncate_inference 1 --max_length_inference 1022\n",
        "\n",
        "import pickle\n",
        "with open('truncate_protein_embeddings.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "NPrg8OANcp8_"
      },
      "id": "NPrg8OANcp8_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/SPLMv2')\n",
        "\n",
        "# residue_level representations\n",
        "!python3 -m utils.generate_seq_embedding --input_seq /content/SPLMv2/dataset/protein.fasta \\\n",
        "--config_path /content/SPLMv2/configs/config_plddtallweight_noseq_rotary_foldseek.yaml \\\n",
        "--checkpoint_path /content/checkpoint_0280000_gvp.pth \\\n",
        "--result_path ./ --out_file truncate_protein_residue_embeddings.pkl \\\n",
        "--truncate_inference 1 --max_length_inference 1022 --residue_level\n",
        "\n",
        "import pickle\n",
        "with open('truncate_protein_residue_embeddings.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "metadata": {
        "id": "08A-HNiANpy3"
      },
      "id": "08A-HNiANpy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocess PDB**\n",
        "\n",
        "First preprocess your PDB files using the provided script; only the resulting HDF5 files can be fed into the S-PLM v2 GVP model.\n"
      ],
      "metadata": {
        "id": "q4rR2G6YZW1l"
      },
      "id": "q4rR2G6YZW1l"
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/SPLMv2/data/preprocess_pdb.py --data /content/SPLMv2/dataset/CATH_4_3_0_non-rep_pdbs/ --save_path /content/CATH_4_3_0_non-rep_gvp/ --max_workers 4"
      ],
      "metadata": {
        "id": "a9GgXHBGTluN"
      },
      "id": "a9GgXHBGTluN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate Structure Embeddings**\n",
        "\n",
        "Use GVP model to produce **residue-level structure embeddings** from **preprocessed HDF5** inputs and save them to `protein_struct_embeddings.pkl`, then quickly print the loaded result for inspection.\n",
        "\n",
        "**Inputs:** `--hdf5_path` (preprocessed data), `--config_path`, `--checkpoint_path`.\n",
        "\n",
        "**Output:** `protein_struct_embeddings.pkl` in the current directory (embeddings per protein/chain).\n",
        "\n",
        "**Note:** You **must preprocess** PDB first, the model only accepts the processed HDF5 tensors.\n"
      ],
      "metadata": {
        "id": "EgL1na1UavMr"
      },
      "id": "EgL1na1UavMr"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/SPLMv2')\n",
        "!python -m utils.generate_struct_embedding \\\n",
        "  --hdf5_path /content/CATH_4_3_0_non-rep_gvp/ \\\n",
        "  --config_path /content/SPLMv2/configs/config_plddtallweight_noseq_rotary_foldseek.yaml \\\n",
        "  --checkpoint_path /content/checkpoint_0280000_gvp.pth \\\n",
        "  --result_path ./ \\\n",
        "  --residue_level\n",
        "\n",
        "import pickle\n",
        "with open('protein_struct_embeddings.pkl', 'rb') as f:\n",
        "    print(pickle.load(f))"
      ],
      "metadata": {
        "id": "0lut4qBBEWRE"
      },
      "id": "0lut4qBBEWRE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluate on CATH**\n",
        "\n",
        "We use a CATH subset to assess the quality of structure embeddings on a clustering task, both through visualizations of the embedding space and through quantitative metrics such as silhouette scores and other clustering-based measures.\n",
        "\n",
        "Build the GVP model structure model, run **CATH** evaluation with preprocessed HDF5 inputs, and save metrics/figures.\n",
        "**Inputs:** `checkpoint_path`, `config_path`, and `cathpath` pointing to the **preprocessed** CATH HDF5 directory.\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "* Instantiates `StructRepresentModel` and sets `out_figure_path`.\n",
        "* Calls `evaluate_with_cath_more_struct(...)` (batch_size=1) to compute clustering/quality metrics (digit-1/2/3, ARI, silhouette).\n",
        "* Prints scores to stdout and writes a summary to `scores.txt` under `out_figure_path`.\n",
        "\n",
        "**Note:** CATH structures must be **preprocessed to HDF5** first (raw PDB won’t be accepted).\n"
      ],
      "metadata": {
        "id": "ZTL9dp1Oc-8y"
      },
      "id": "ZTL9dp1Oc-8y"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/SPLMv2')\n",
        "from pathlib import Path\n",
        "from utils.generate_struct_embedding import StructRepresentModel\n",
        "from cath_with_struct import evaluate_with_cath_more_struct\n",
        "\n",
        "checkpoint_path = \"/content/checkpoint_0280000_gvp.pth\"\n",
        "config_path = \"/content/SPLMv2/configs/config_plddtallweight_noseq_rotary_foldseek.yaml\"\n",
        "model, device, configs = StructRepresentModel(config_path=config_path, checkpoint_path=checkpoint_path)\n",
        "out_figure_path = os.path.join(config_path.split(\".yaml\")[0], \"CATH_test_release\")\n",
        "Path(out_figure_path).mkdir(parents=True, exist_ok=True)\n",
        "cathpath = \"/content/CATH_4_3_0_non-rep_gvp/\"\n",
        "scores_cath = evaluate_with_cath_more_struct(\n",
        "    out_figure_path,\n",
        "    device=device,\n",
        "    batch_size=1,\n",
        "    model=model,\n",
        "    cathpath=cathpath,\n",
        "    configs=configs\n",
        "    )\n",
        "print(f\"gvp_digit_num_1:{scores_cath[0]:.4f}({scores_cath[1]:.4f})\\tgvp_digit_num_2:{scores_cath[4]:.4f}({scores_cath[5]:.4f})\\tgvp_digit_num_3:{scores_cath[8]:.4f}({scores_cath[9]:.4f})\\n\")\n",
        "print(f\"gvp_digit_num_1_ARI:{scores_cath[2]}\\tgvp_digit_num_2_ARI:{scores_cath[6]}\\tgvp_digit_num_3_ARI:{scores_cath[10]}\\n\")\n",
        "print(f\"gvp_digit_num_1_silhouette:{scores_cath[3]}\\tgvp_digit_num_2_silhouette:{scores_cath[7]}\\tgvp_digit_num_3_silhouette:{scores_cath[11]}\\n\")\n",
        "\n",
        "with open(os.path.join(out_figure_path, 'scores.txt'), 'w') as file:\n",
        "    file.write(f\"gvp_digit_num_1:{scores_cath[0]:.4f}({scores_cath[1]:.4f})\\tgvp_digit_num_2:{scores_cath[3]:.4f}({scores_cath[4]:.4f})\\tgvp_digit_num_3:{scores_cath[6]:.4f}({scores_cath[7]:.4f})\\n\")\n",
        "    file.write(f\"gvp_digit_num_1_ARI:{scores_cath[2]}\\tgvp_digit_num_2_ARI:{scores_cath[5]}\\tgvp_digit_num_3_ARI:{scores_cath[8]}\\n\")\n",
        "    file.write(f\"gvp_digit_num_1_silhouette:{scores_cath[3]}\\tgvp_digit_num_2_silhouette:{scores_cath[7]}\\tgvp_digit_num_3_silhouette:{scores_cath[11]}\\n\")\n"
      ],
      "metadata": {
        "id": "-Jb7jzydPgtm"
      },
      "id": "-Jb7jzydPgtm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J0Zx6LDn_ZEu",
        "3efac75f"
      ],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}